{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Goal\n",
    "Understand why natural language processing and text representation are important, the different ways to represent text, and how to implement a few simple textual representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Libraries\n",
    "We'll be using the gensim library to learn word embeddings. The commented out lines below are for installing gensim through anaconda and python respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT TEST RUN\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.summarization.textcleaner import split_sentences, tokenize_by_word\n",
    "# conda install -c anaconda gensim\n",
    "# pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing that our code works\n",
    "This code reads in the raw .xml file from pubmed and parses out some abstracts into a more readable text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT TEST RUN\n",
    "\n",
    "# Process the original pubmed download. This is just so you can see how it's done. We won't work with the xml file.\n",
    "n_abs = 0\n",
    "with open(\"data/pubmed_sample_test.txt\", \"w\") as outfile:\n",
    "    with open(\"data/pubmed20n0001.xml\", \"r\") as pubmed_file:\n",
    "        for line in pubmed_file:\n",
    "            if \"<AbstractText>\" in line:\n",
    "                line = line.strip()# remove leading and trailing whitespace\n",
    "                line = line.replace(\"<AbstractText>\", \"\").replace(\"</AbstractText>\", \"\")# these strings identify \n",
    "                    # when an abstract is present in the xml file.\n",
    "                outfile.write(line + \"\\n\")# write the text to the text file.\n",
    "                n_abs += 1\n",
    "print(n_abs, \"abstracts processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data from file\n",
    "The data is a text file of abstracts separated by new lines. We'll read this data into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(abstract_list[i])\n",
    "    print(\"***************************************************\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process our data\n",
    "The next step is processing our abstracts into sentences. Word2vec can work with either sentences to learn the context around words, or with entire documents (abstracts). This is a design choice and up to you. In the next section process the abstracts into sentences and store them in a list where you have one sentence per element in the list. The documentation is in (https://radimrehurek.com/gensim/summarization/textcleaner.html) for the function `split_sentences` which we'll be using\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(sentence_list[i])\n",
    "    print(\"***************************************************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim expects each sentence or document as a list of words\n",
    "Gensim works with sentences or documents not as strings, but as lists of words or tokens. So for each sentence and for each abstract we need to convert it into a list of tokens/words. We can use the function `tokenize_by_word`. See documentation (https://radimrehurek.com/gensim/summarization/textcleaner.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One more step. Word2Vec expects a lists of text, where each text is a list of tokens, or words.\n",
    "abstract_list_tokenized = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list_tokenized = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "abstract_list_tokenized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training word embeddings\n",
    "Using the function `Word2Vec` from gensim we can now train word embeddings. The documentation is in (https://radimrehurek.com/gensim/models/word2vec.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_abstract = Word2Vec(\n",
    "\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sentence = Word2Vec(\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore trained word embeddings\n",
    "Now we  can explore the word embeddings. Take a look at the embeddings. How many are there? How big are they? Do they make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model_abstract.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embeddings.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How else can we explore word embeddings?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
